{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5380d5df",
   "metadata": {
    "id": "5380d5df"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x7zJSvw7VfHr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7zJSvw7VfHr",
    "outputId": "8f8ea565-d965-445e-eb73-247cd1c247c0"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('Hamza_Custom_Data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd10b90d",
   "metadata": {
    "id": "cd10b90d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a30730a",
   "metadata": {
    "id": "7a30730a"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,Flatten,MaxPooling2D,Dropout,Dense,Activation,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bdede3d",
   "metadata": {
    "id": "1bdede3d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b09238",
   "metadata": {
    "id": "84b09238"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f62a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('Hamza_Custom_Data/CT_Scan/Covid/CT_COVID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe0c3b7",
   "metadata": {
    "id": "cbe0c3b7"
   },
   "outputs": [],
   "source": [
    "# selecting random number of images from multiple folders and copying it to the destination folder\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "# ctscan_covid_data = random.sample(glob.glob(\"Hamza_Custom_Data/CT_Scan/Covid/CT_COVID/*.png\"), 330)\n",
    "# ctscan_normal_data = random.sample(glob.glob(\"Hamza_Custom_Data/CT_Scan/Normal/CT_nonCOVID/*\"), 330)\n",
    "\n",
    "# ultrasound_covid_data = random.sample(glob.glob(\"Hamza_Custom_Data/ultrasound/Covid/*\"), 330)\n",
    "# ultrasound_normal_data = random.sample(glob.glob(\"Hamza_Custom_Data/ultrasound/Normal/*.jpg\"), 330)\n",
    "\n",
    "# xray_covid_data = random.sample(glob.glob(\"Hamza_Custom_Data/X_ray/Covid/*.png\"), 330)\n",
    "# xray_normal_data = random.sample(glob.glob(\"Hamza_Custom_Data/X_ray/Normal/*.png\"), 330)\n",
    "\n",
    "ctscan_covid_data = random.sample(glob.glob(\"Hamza_Custom_Data/CT_Scan/Covid/*.png\"), 330)\n",
    "ctscan_normal_data = random.sample(glob.glob(\"Hamza_Custom_Data/CT_Scan/Normal/*\"), 330)   \n",
    "\n",
    "ultrasound_covid_data = random.sample(glob.glob(\"Hamza_Custom_Data/Ultra_Sound/Covid/*\"), 330)\n",
    "ultrasound_normal_data = random.sample(glob.glob(\"Hamza_Custom_Data/Ultra_Sound/Normal/*.jpg\"), 330)\n",
    "\n",
    "xray_covid_data = random.sample(glob.glob(\"Hamza_Custom_Data/X_Ray/Covid/*.png\"), 330)\n",
    "xray_normal_data = random.sample(glob.glob(\"Hamza_Custom_Data/X_Ray/Normal/*.png\"), 330)\n",
    "\n",
    "covid_list=[ctscan_covid_data,ultrasound_covid_data,xray_covid_data]\n",
    "\n",
    "normal_list = [ctscan_normal_data, ultrasound_normal_data,xray_normal_data]\n",
    "\n",
    "if os.path.isdir('Data/Covid'):\n",
    "    shutil.rmtree('Data/Covid')\n",
    "    \n",
    "if os.path.isdir('Data/Normal'):\n",
    "    shutil.rmtree('Data/Normal')\n",
    "    \n",
    "# if os.path.isdir('/content/drive/MyDrive/Data/X_Ray'):\n",
    "#     shutil.rmtree('/content/drive/MyDrive/Data/X_Ray')\n",
    "\n",
    "for data in covid_list:\n",
    "    dest = 'Data/Covid'\n",
    "    if not os.path.isdir('Data/Covid'):\n",
    "        os.makedirs(dest)\n",
    "    for imgs in data:\n",
    "        shutil.copy(imgs, dest)\n",
    "\n",
    "for data in normal_list:\n",
    "    dest = 'Data/Normal' \n",
    "    if not os.path.isdir('Data/Normal'):\n",
    "          os.makedirs(dest)\n",
    "    for imgs in data:\n",
    "        shutil.copy(imgs, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7d589a7",
   "metadata": {
    "id": "f7d589a7"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data_root: str, *, test_size: float, img_size: int, seed: int = 0) -> None:\n",
    "        self.label2index = {}\n",
    "        self.index2label = {}\n",
    "        \n",
    "        # Discover the class label names.\n",
    "        class_labels = os.listdir(data_root)\n",
    "        self.nclasses = len(class_labels)\n",
    "        X, y = [], []\n",
    "        \n",
    "        for label_index, label in enumerate(class_labels):\n",
    "            # Load the images for this class label.\n",
    "            self.label2index[label_index] = label\n",
    "            self.index2label[label] = label_index\n",
    "            \n",
    "            img_names = os.listdir(os.path.join(data_root, label))\n",
    "            for img_name in img_names:\n",
    "                img_path = os.path.join(data_root, label, img_name)\n",
    "                img = load_img(img_path, target_size=(img_size, img_size, 3))\n",
    "                X.append(img_to_array(img))\n",
    "                y.append(label_index)\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        one_hot_y = to_categorical(y, num_classes=self.nclasses)\n",
    "        \n",
    "        # Make a stratified split.\n",
    "        self.X, self.X_test, self.labels, self.labels_test, self.y, self.y_test = train_test_split(\n",
    "            X, y, one_hot_y, test_size=test_size, random_state=seed, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70ad25ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70ad25ed",
    "outputId": "ceebd03a-c68e-494f-8bb7-4d52bebfb574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1386, 224, 224, 3) (1386, 2)\n"
     ]
    }
   ],
   "source": [
    "# 660 * 0.7 = 462 \n",
    "# X shape in 3 dimensions\n",
    "# Y has 2 classes (Covid, Normal)\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "data = Dataset(\"Data/\", test_size=0.3, img_size=224)\n",
    "print(data.X.shape, data.y.shape)\n",
    "\n",
    "# Normal = Dataset(\"/content/drive/MyDrive/Data/\", test_size=0.3, img_size=224)\n",
    "# print(Normal.X.shape, Normal.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "FuhCijq7mxKy",
   "metadata": {
    "id": "FuhCijq7mxKy"
   },
   "outputs": [],
   "source": [
    "# feature extractor model resnet 101 v2\n",
    "model = hub.KerasLayer(\"https://tfhub.dev/google/bit/m-r101x1/1\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1dcb4c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "b1dcb4c7",
    "outputId": "4637b29a-a5fb-4a4a-d0e0-55b8a583f993",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1386, 1280) (594, 1280)\n"
     ]
    }
   ],
   "source": [
    "covid_embedding = model(data.X)\n",
    "covid_test_embedding = model(data.X_test)\n",
    "print(covid_embedding.shape, covid_test_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21d654e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor model mobinet v2\n",
    "model_mobinet = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcbe5836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1386, 1280) (594, 1280)\n"
     ]
    }
   ],
   "source": [
    "covid_embedding_mob = model_mobinet(data.X)\n",
    "covid_test_embedding_mob = model_mobinet(data.X_test)\n",
    "print(covid_embedding_mob.shape, covid_test_embedding_mob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb41136e",
   "metadata": {
    "id": "cb41136e"
   },
   "outputs": [],
   "source": [
    "def make_model(nclasses: int):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = 16, kernel_size = (3,3), padding='same', input_shape=(2,2,320), activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Conv2D(24, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Conv2D(32, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Conv2D(48, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Conv2D(64, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128,activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128,activation=\"relu\"))\n",
    "    model.add(Dense(nclasses, activation=\"sigmoid\"))\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f48372d3",
   "metadata": {
    "id": "f48372d3"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(nclasses, X, y, X_test, y_test, *, epochs: int, batch_size: int, learning_rate: float, \n",
    "                   **model_params) -> tuple:\n",
    "    \n",
    "    # Math to compute the learning rate schedule. We will divide our\n",
    "    # learning rate by a factor of 10 every 30% of the optimizer's\n",
    "    # total steps.\n",
    "    steps_per_epoch = math.ceil(len(X) / batch_size)\n",
    "    third_of_total_steps = math.floor(epochs * steps_per_epoch / 3)\n",
    "    \n",
    "    # Make and compile the model.\n",
    "    #model = model_maker(nclasses, **model_params)\n",
    "    model = make_model(nclasses)\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=ExponentialDecay(\n",
    "                learning_rate,\n",
    "                decay_steps=third_of_total_steps,\n",
    "                decay_rate=0.1,\n",
    "                staircase=True\n",
    "            )\n",
    "        ),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    # Train the model on the training set and evaluate it on the test set.\n",
    "    history = model.fit(X, y, batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "    _, train_acc = model.evaluate(X, y, batch_size=batch_size, verbose=0)\n",
    "    _, test_acc = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    return model, history, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a54886ee",
   "metadata": {
    "id": "a54886ee"
   },
   "outputs": [],
   "source": [
    "def cv_evaluate_model(\n",
    "    X, y, labels, *, nfolds: int, nrepeats: int, epochs: int, batch_size: int,\n",
    "    learning_rate: float, model_maker, verbose: bool = True, seed: int = 0,\n",
    "    **model_params\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Performs `nfolds` cross-validated training and evaluation of a\n",
    "    model hyperparameter configuration. Returns a dictionary of\n",
    "    statistics about the outcome of the cross-validated experiment.\n",
    "    \"\"\"\n",
    "    _, nclasses = y.shape\n",
    "    train_accs, test_accs = [], []\n",
    "    \n",
    "    # Train and evaluate the model for each fold.\n",
    "    for train_index, test_index in tqdm(\n",
    "        RepeatedStratifiedKFold(\n",
    "            n_splits=nfolds, n_repeats=nrepeats, random_state=seed\n",
    "        ).split(X, labels),\n",
    "        total=nfolds*nrepeats, disable=not verbose\n",
    "    ):\n",
    "        \n",
    "        # Select the data for this fold.\n",
    "        X_train_fold = tf.gather(X, train_index) \n",
    "        y_train_fold = tf.gather(y, train_index)\n",
    "        X_test_fold = tf.gather(X, test_index)\n",
    "        y_test_fold = tf.gather(y, test_index)\n",
    "        \n",
    "        # Train and evaluate the model.\n",
    "        _, _, train_acc, test_acc = evaluate_model(\n",
    "            nclasses,\n",
    "            X_train_fold,\n",
    "            y_train_fold,\n",
    "            X_test_fold,\n",
    "            y_test_fold,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            model_maker=model_maker,\n",
    "            **model_params\n",
    "        )\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "    \n",
    "    # Aggregate.\n",
    "    results = {\n",
    "        \"train_mean\": np.mean(train_accs),\n",
    "        \"train_std\": np.std(train_accs),\n",
    "        \"test_mean\": np.mean(test_accs),\n",
    "        \"test_std\": np.std(test_accs)\n",
    "    }\n",
    "    \n",
    "    # Report.\n",
    "    if verbose:\n",
    "        print(\n",
    "            tabulate(\n",
    "                [\n",
    "                    [\"Train\", results[\"train_mean\"], results[\"train_std\"]],\n",
    "                    [\"Test\", results[\"test_mean\"], results[\"test_std\"]]\n",
    "                ],\n",
    "                headers=[\"Set\", \"Accuracy\", \"Standard Deviation\"]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3abc588",
   "metadata": {
    "id": "b3abc588"
   },
   "outputs": [],
   "source": [
    "X_list = [covid_embedding]\n",
    "y_list = [data.y]\n",
    "data_labels = [data.labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac2e4995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1386, 1280])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0454dea",
   "metadata": {
    "id": "b0454dea",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1386, 2, 2, 320)\n",
      "(1386, 2, 2, 320)\n"
     ]
    }
   ],
   "source": [
    "# Chaning shape for CNN2D\n",
    "# for 2048 - > [-1,2,2,512]\n",
    "# for 1280 - > [-1,2,2,320]\n",
    "\n",
    "covid_embedding2d = tf.reshape(covid_embedding, [-1,2,2,320])\n",
    "print(covid_embedding2d.shape)\n",
    "\n",
    "covid_embedding2d_mob = tf.reshape(covid_embedding_mob, [-1,2,2,320])\n",
    "print(covid_embedding2d_mob.shape)\n",
    "\n",
    "X_list = [covid_embedding2d]\n",
    "X_list_mob = [covid_embedding2d_mob]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e638d6",
   "metadata": {
    "id": "30e638d6"
   },
   "source": [
    "### Model Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f819ea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 2, 2, 16)          46096     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 2, 2, 16)          64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 2, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 24)          3480      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 24)          96        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 2, 2, 24)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 32)          6944      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 2, 2, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 48)          13872     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 2, 2, 48)          192       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 2, 2, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 2, 2, 64)          27712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 148,506\n",
      "Trainable params: 148,138\n",
      "Non-trainable params: 368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_class = y_list[0].shape[1]\n",
    "model_cnn = make_model(n_class)\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daabab8",
   "metadata": {},
   "source": [
    "### Results with Resnet 101 v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e40368e",
   "metadata": {
    "id": "5e40368e"
   },
   "outputs": [],
   "source": [
    "model_evaluate_params = {\n",
    "    \"X\": X_list[0],\n",
    "    \"y\": y_list[0],\n",
    "    \"labels\": data_labels[0],\n",
    "    \"nfolds\": 10,\n",
    "    \"nrepeats\": 3,\n",
    "    \"model_maker\": make_model,\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 32,\n",
    "    \"verbose\": False,\n",
    "    \"learning_rate\": 3e-3 #0.003\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b71e3623",
   "metadata": {
    "id": "b71e3623",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                             | 1/30 [03:24<1:39:04, 205.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-506ddb374bcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m _ = cv_evaluate_model(\n\u001b[0m\u001b[0;32m      2\u001b[0m     **{\n\u001b[0;32m      3\u001b[0m         \u001b[1;33m**\u001b[0m\u001b[0mctscan_model_evaluate_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;34m\"verbose\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     }\n",
      "\u001b[1;32m<ipython-input-31-fb6ad1eaee4c>\u001b[0m in \u001b[0;36mcv_evaluate_model\u001b[1;34m(X, y, labels, nfolds, nrepeats, epochs, batch_size, learning_rate, model_maker, verbose, seed, **model_params)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Train and evaluate the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         _, train_acc, test_acc = evaluate_model(\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mnclasses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mX_train_fold\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-89224664929e>\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(nclasses, X, y, X_test, y_test, epochs, batch_size, learning_rate, **model_params)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Train the model on the training set and evaluate it on the test set.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1494\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1495\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1496\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    694\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 719\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3117\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3118\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3119\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3120\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0;32m   3121\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_ = cv_evaluate_model(\n",
    "    **{\n",
    "        **model_evaluate_params,\n",
    "        \"verbose\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5904c",
   "metadata": {},
   "source": [
    "### Results with Mobinet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "z3aPTroE-2Gs",
   "metadata": {
    "id": "z3aPTroE-2Gs"
   },
   "outputs": [],
   "source": [
    "model_evaluate_params_mob = {\n",
    "    \"X\": X_list_mob[0],\n",
    "    \"y\": y_list[0],\n",
    "    \"labels\": data_labels[0],\n",
    "    \"nfolds\": 10,\n",
    "    \"nrepeats\": 3,\n",
    "    \"model_maker\": make_model,\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 32,\n",
    "    \"verbose\": False,\n",
    "    \"learning_rate\": 3e-3 #0.003\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "252a6a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 30/30 [1:00:55<00:00, 121.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set      Accuracy    Standard Deviation\n",
      "-----  ----------  --------------------\n",
      "Train    0.99984            0.000599645\n",
      "Test     0.839582           0.0335461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = cv_evaluate_model(\n",
    "    **{\n",
    "        **model_evaluate_params_mob,\n",
    "        \"verbose\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5321696a",
   "metadata": {},
   "source": [
    "### Graph Method for Resnet 101 v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_value = X_list[0]\n",
    "y_value = y_list[0]\n",
    "labels = data_labels[0]\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "nclasses = y_value.shape[1]\n",
    "nfolds = 10\n",
    "nrepeats = 3\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 3e-3 #0.003\n",
    "\n",
    "# Train and evaluate the model for each fold.\n",
    "for train_index, test_index in tqdm(\n",
    "    RepeatedStratifiedKFold(n_splits = nfolds, n_repeats = nrepeats, random_state = 0).split(X, labels), \\\n",
    "    total = nfolds*nrepeats, disable = not True #(verbose - True)\n",
    "):\n",
    "\n",
    "    # Select the data for this fold.\n",
    "    X_train = tf.gather(X_value, train_index) \n",
    "    y_train = tf.gather(y_value, train_index)\n",
    "    X_test = tf.gather(X_value, test_index)\n",
    "    y_test = tf.gather(y_value, test_index)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    #Define Model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = 16, kernel_size = (3,3), padding='same', input_shape=(2,2,320), activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Conv2D(24, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Conv2D(32, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Conv2D(48, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Conv2D(64, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128,activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128,activation=\"relu\"))\n",
    "    model.add(Dense(nclasses, activation=\"sigmoid\"))\n",
    "\n",
    "    #Learning Rate\n",
    "    steps_per_epoch = math.ceil(len(X_train) / batch_size)\n",
    "    third_of_total_steps = math.floor(epochs * steps_per_epoch / 3)\n",
    "    \n",
    "    # Train and evaluate the model.\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=ExponentialDecay(\n",
    "                learning_rate,\n",
    "                decay_steps=third_of_total_steps,\n",
    "                decay_rate=0.1,\n",
    "                staircase=True\n",
    "            )\n",
    "        ),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    # Train the model on the training set and evaluate it on the test set.\n",
    "    history = (model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, verbose=1, \\\n",
    "                         validation_data=(X_test, y_test)))\n",
    "    train_loss, train_acc = model.evaluate(X_train, y_train, batch_size = batch_size, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, batch_size = batch_size, verbose=0)\n",
    "    \n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "# Aggregate.\n",
    "results = {\n",
    "    \"Train_Acc\": np.mean(train_accs),\n",
    "    \"Train_std\": np.std(train_accs),\n",
    "    \"Test_Acc\": np.mean(test_accs),\n",
    "    \"Test_std\": np.std(test_accs)\n",
    "}\n",
    "\n",
    "# Report.\n",
    "if verbose:\n",
    "    print(\n",
    "        tabulate(\n",
    "            [\n",
    "                [\"Train\", results[\"Train_Acc\"], results[\"Train_std\"]],\n",
    "                [\"Test\", results[\"Test_Acc\"], results[\"Test_std\"]]\n",
    "            ],\n",
    "            headers=[\"Set\", \"Accuracy\", \"Standard Deviation\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca2572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting graph by using values of last epoch for graphs\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.ylabel('Accuracy value (%)')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.ylabel('Loss value')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab3a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68043f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All in one Graph (loss and accuracy)\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.plot(history.history[\"loss\"]) \n",
    "plt.plot(history.history[\"val_loss\"]) \n",
    "plt.title(\"Model Evaluation\")\n",
    "plt.ylabel(\"Value (%)\")\n",
    "plt.xlabel(\"No. of Epochs\")\n",
    "plt.legend([\"Training Accuracy\",\"Validation Accuracy\",\"Training loss\",\"Validation loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eda750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These confussion matrix are not correct\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test1 = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(accuracy_score(y_test1, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589c367",
   "metadata": {},
   "source": [
    "### Graph Method for Mobinet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f1a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_value = X_list_mob[0]\n",
    "y_value = y_list[0]\n",
    "labels = data_labels[0]\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "nclasses = y_value.shape[1]\n",
    "nfolds = 10\n",
    "nrepeats = 3\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 3e-3 #0.003\n",
    "\n",
    "# Train and evaluate the model for each fold.\n",
    "for train_index, test_index in tqdm(\n",
    "    RepeatedStratifiedKFold(n_splits = nfolds, n_repeats = nrepeats, random_state = 0).split(X, labels), \\\n",
    "    total = nfolds*nrepeats, disable = not True #(verbose - True)\n",
    "):\n",
    "\n",
    "    # Select the data for this fold.\n",
    "    X_train = tf.gather(X_value, train_index) \n",
    "    y_train = tf.gather(y_value, train_index)\n",
    "    X_test = tf.gather(X_value, test_index)\n",
    "    y_test = tf.gather(y_value, test_index)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    #Define Model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = 16, kernel_size = (3,3), padding='same', input_shape=(2,2,320), activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Conv2D(24, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Conv2D(32, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Conv2D(48, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Conv2D(64, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(1,1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128,activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128,activation=\"relu\"))\n",
    "    model.add(Dense(nclasses, activation=\"sigmoid\"))\n",
    "\n",
    "    #Learning Rate\n",
    "    steps_per_epoch = math.ceil(len(X_train) / batch_size)\n",
    "    third_of_total_steps = math.floor(epochs * steps_per_epoch / 3)\n",
    "    \n",
    "    # Train and evaluate the model.\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=ExponentialDecay(\n",
    "                learning_rate,\n",
    "                decay_steps=third_of_total_steps,\n",
    "                decay_rate=0.1,\n",
    "                staircase=True\n",
    "            )\n",
    "        ),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    # Train the model on the training set and evaluate it on the test set.\n",
    "    history = (model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, verbose=1, \\\n",
    "                         validation_data=(X_test, y_test)))\n",
    "    train_loss, train_acc = model.evaluate(X_train, y_train, batch_size = batch_size, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, batch_size = batch_size, verbose=0)\n",
    "    \n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "# Aggregate.\n",
    "results = {\n",
    "    \"Train_Acc\": np.mean(train_accs),\n",
    "    \"Train_std\": np.std(train_accs),\n",
    "    \"Test_Acc\": np.mean(test_accs),\n",
    "    \"Test_std\": np.std(test_accs)\n",
    "}\n",
    "\n",
    "# Report.\n",
    "if verbose:\n",
    "    print(\n",
    "        tabulate(\n",
    "            [\n",
    "                [\"Train\", results[\"Train_Acc\"], results[\"Train_std\"]],\n",
    "                [\"Test\", results[\"Test_Acc\"], results[\"Test_std\"]]\n",
    "            ],\n",
    "            headers=[\"Set\", \"Accuracy\", \"Standard Deviation\"]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98eb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting graph by using values of last epoch for graphs\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.ylabel('Accuracy value (%)')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.ylabel('Loss value')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a494744",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All in one Graph (loss and accuracy)\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.plot(history.history[\"loss\"]) \n",
    "plt.plot(history.history[\"val_loss\"]) \n",
    "plt.title(\"Model Evaluation\")\n",
    "plt.ylabel(\"Value (%)\")\n",
    "plt.xlabel(\"No. of Epochs\")\n",
    "plt.legend([\"Training Accuracy\",\"Validation Accuracy\",\"Training loss\",\"Validation loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04718600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These confussion matrix are not correct\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test1 = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(accuracy_score(y_test1, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Hamza_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
